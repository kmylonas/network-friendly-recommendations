{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCC4XJnFe8MS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctMaALpJnOw6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions.categorical import Categorical\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "a = 0.9 #probability that user picks a recommendation, given that he continues\n",
        "q = 0.1 #probability that the user exits (terminates) the session\n",
        "\n",
        "K = 50 #num of videos in platform (represents the states - numbered from 0 to K-1)\n",
        "N = 2  #num of recommendations\n",
        "u_min = 0.1 #relevance threshold\n",
        "\n",
        "\n",
        "U = np.random.rand(K, K) #relevance matrix\n",
        "np.fill_diagonal(U, 0)\n",
        "U_bool = U > u_min #0 if j is irrelevant to i, 1 if j is relevant to i\n",
        "C = np.random.choice([0, 1], size=K, p=[0.8, 0.2])\n",
        "\n",
        "# for testing purposes K = 20\n",
        "# U = np.random.uniform(0.001, 1, size=(K, K))\n",
        "# np.fill_diagonal(U, 0)\n",
        "# U_bool = U > u_min #0 if j is irrelevant to i, 1 if j is relevant to i\n",
        "# C = np.array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 ,0 ,0, 0, 0, 0, 0, 0])\n",
        "\n",
        "\n",
        "hit_table = torch.tensor(U_bool & C).float()\n",
        "cached_tensor = torch.tensor(C).float()\n",
        "\n",
        "\n",
        "actions = list(combinations(range(K), N))\n",
        "num_of_actions = len(actions)\n",
        "\n",
        "\n",
        "\n",
        "##Neural Net\n",
        "def mlp(num_of_actions):\n",
        "   return nn.Sequential(\n",
        "            nn.Linear(K, 2*K),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*K, num_of_actions),\n",
        "            nn.Identity()\n",
        "   )\n",
        "\n",
        "\n",
        "def train(epochs=50, batch_size=1000, learning_rate=0.01, eval=False, eval_episodes=1000):\n",
        "\n",
        "\n",
        "    logits_net = mlp(num_of_actions) #policy net\n",
        "    v_approx_net = nn.Sequential(    #value function approximation net\n",
        "            nn.Linear(K, K),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(K, 1),\n",
        "            nn.ReLU()\n",
        "    )\n",
        "\n",
        "    def get_policy(obs):\n",
        "\n",
        "\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    def get_action(obs):\n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    def compute_loss(obs, act, weights):\n",
        "        logp = get_policy(obs).log_prob(act)\n",
        "        return -(logp * weights).mean()\n",
        "\n",
        "    def compute_v_loss(pred, target):\n",
        "      se = (pred - target) ** 2\n",
        "      mse = se.mean()\n",
        "      return mse\n",
        "\n",
        "\n",
        "    def reward_to_go(rews):\n",
        "      n = len(rews)\n",
        "      rtgs = np.zeros_like(rews)\n",
        "      for i in reversed(range(n)):\n",
        "          rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
        "      return rtgs\n",
        "\n",
        "\n",
        "    def evaluate_net():\n",
        "      total_reward = 0\n",
        "      for i in range(eval_episodes):\n",
        "        state = reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "\n",
        "          #Case 1: Input is relevance row\n",
        "          obs = torch.tensor(U[state], dtype=torch.float64).float()\n",
        "          #Case 2: Input is the concatenation of cached and U[state]\n",
        "          # obs = torch.cat((torch.as_tensor(U[state]), cached_tensor), dim=0).float()\n",
        "\n",
        "          act = get_action(obs)\n",
        "          state, rew, done = env(state, actions[act])\n",
        "\n",
        "          total_reward += rew\n",
        "\n",
        "      print(f'Policy network achieved: {total_reward / eval_episodes}')\n",
        "\n",
        "\n",
        "      return\n",
        "\n",
        "    v_optimizer = Adam(v_approx_net.parameters(), lr=learning_rate)\n",
        "    optimizer = Adam(logits_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    pg_epoch_avg_ret = []\n",
        "\n",
        "    def train_one_epoch(eval=False):\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "        batch_rews_to_go = []   #for G_t at each timestep at each episode\n",
        "\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        state = reset()       # first obs comes from starting distribution\n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "        ep_v_s = []             #list with the value function estimations of this episode\n",
        "        episode_num = 0         #keep track of how many episodes has been played\n",
        "\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        while True:\n",
        "\n",
        "            # save obs\n",
        "            #Case 1: Input is relevance row\n",
        "            obs = torch.tensor(U[state], dtype=torch.float64).float()\n",
        "            #Case 2: Input is the concatenation of cached and U[state]\n",
        "            # obs = torch.cat((torch.as_tensor(U[state]), cached_tensor), dim=0).float()\n",
        "\n",
        "            batch_obs.append(obs.tolist().copy())\n",
        "\n",
        "            # act in the environment\n",
        "            act = get_action(obs)\n",
        "\n",
        "            v_s = v_approx_net(obs)\n",
        "\n",
        "            state, rew, done = env(state, actions[act]) #obs is one input vector for the neural net. It is the state.\n",
        "\n",
        "            # save action, reward, and estimation\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "            ep_v_s.append(v_s.item())\n",
        "\n",
        "\n",
        "            if done:\n",
        "\n",
        "                episode_num += 1\n",
        "                # if episode is over, record info about episode\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "\n",
        "                rew_to_go = reward_to_go(ep_rews)\n",
        "                advantage = rew_to_go - np.array(ep_v_s)\n",
        "\n",
        "                batch_rews_to_go += list(rew_to_go) #will be used for fitting baseline\n",
        "\n",
        "                # # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
        "                # batch_weights += list(reward_to_go(ep_rews))\n",
        "\n",
        "                #the weight is the advantage estimation\n",
        "                batch_weights += list(advantage)\n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews, v_s, ep_v_s = reset(), False, [], [], []\n",
        "\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if episode_num == batch_size:\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #one step value function approximation network update\n",
        "        v_optimizer.zero_grad()\n",
        "        batch_estimates = v_approx_net(torch.as_tensor(batch_obs, dtype=torch.float32))\n",
        "        criterion = nn.MSELoss()\n",
        "        v_loss = criterion(batch_estimates, torch.as_tensor(batch_rews_to_go, dtype=torch.float32).unsqueeze(1))\n",
        "        v_loss.backward()\n",
        "        v_optimizer.step()\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        pg_epoch_avg_ret.append(np.mean(batch_rets))\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
        "\n",
        "\n",
        "    plot_curves(pg_epoch_avg_ret)\n",
        "\n",
        "    # evaluate\n",
        "    if eval: evaluate_net()\n",
        "\n",
        "\n",
        "    return pg_epoch_avg_ret\n",
        "\n",
        "\n",
        "##########Q LEARNING#############\n",
        "def q_learning(gamma = 1, alpha=0.1, e_t=0.1, epochs=20, batch_size=5000,eval=True, eval_episodes=1000):\n",
        "\n",
        "    def get_action(Q):\n",
        "\n",
        "\n",
        "      return actions[a]\n",
        "\n",
        "    def eval_q_learning(pi):\n",
        "      total_reward = 0\n",
        "      for i in range(eval_episodes):\n",
        "        state = reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "          act = pi[state, :]\n",
        "          state, rew, done = env(state, act)\n",
        "          total_reward += rew\n",
        "\n",
        "      print(f'Q learning achieved: {total_reward / eval_episodes}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Starting Q-learning...\")\n",
        "    Q = np.zeros((K, num_of_actions))\n",
        "\n",
        "    epoch_avg_rew = []\n",
        "    epoch_rets = []\n",
        "    epoch_progress = 0\n",
        "    completed_epochs = 0\n",
        "    episode_num = 0\n",
        "\n",
        "\n",
        "    while completed_epochs < epochs:\n",
        "        ep_ret = 0\n",
        "        s = np.random.randint(0, K)\n",
        "\n",
        "        while True:\n",
        "\n",
        "            explore = np.random.binomial(1, e_t)\n",
        "            if(explore == 1):\n",
        "                a = np.random.randint(0, num_of_actions) #choose a random action to play at this round\n",
        "            else:\n",
        "                a = np.argmax(Q[s,:]) #pick the best action\n",
        "            action = actions[a]\n",
        "            next_state, reward, done = env(s, action)\n",
        "            ep_ret += reward\n",
        "\n",
        "\n",
        "            if done: #finish episode\n",
        "                epoch_progress += 1\n",
        "                epoch_rets.append(ep_ret) #store the total reward of the episode\n",
        "                if epoch_progress == batch_size: #epoch finished\n",
        "                    epoch_avg_rew.append(np.mean(epoch_rets))\n",
        "                    epoch_rets = []\n",
        "                    completed_epochs += 1\n",
        "                    epoch_progress = 0\n",
        "\n",
        "                break\n",
        "            else:\n",
        "                Q[s, a] += alpha*(reward + gamma*np.max(Q[next_state, :]) - Q[s, a])\n",
        "\n",
        "            s = next_state\n",
        "\n",
        "\n",
        "        episode_num += 1\n",
        "\n",
        "\n",
        "    pi = generate_improved_policy(Q)\n",
        "    plot_curves(epoch_avg_rew)\n",
        "\n",
        "    if eval: eval_q_learning(pi)\n",
        "\n",
        "    return epoch_avg_rew\n",
        "\n",
        "\n",
        "\n",
        "##### FUNCTIONS #######\n",
        "\n",
        "def plot_curves(pg_epoch_avg_ret):\n",
        "  plt.figure()\n",
        "  plt.title(\"Reward Per Episode vs. Episode\")\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Reward per episode\")\n",
        "  plt.plot(np.arange(1,len(pg_epoch_avg_ret)+1), pg_epoch_avg_ret, color='b')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_pg_q_same_graph(pg_epoch_avg_ret, q_epoch_avg_ret):\n",
        "  #Plot q learning and pg on the same graph\n",
        "  plt.plot(np.arange(1,len(pg_epoch_avg_ret)+1), pg_epoch_avg_ret, label='PG')\n",
        "\n",
        "  # Plot the second set of data and label it\n",
        "  plt.plot(np.arange(1,len(pg_epoch_avg_ret)+1), q_epoch_avg_ret, label='Q')\n",
        "\n",
        "  # Add a legend\n",
        "  plt.legend()\n",
        "\n",
        "  # Add labels and a title\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Average episode return')\n",
        "  plt.title('Average episode return vs Epoch')\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def reset():\n",
        "    random_state = np.random.randint(0,K)\n",
        "    return random_state\n",
        "    # return one_hot_tensor(random_state)\n",
        "\n",
        "\n",
        "def env(state, action):\n",
        "\n",
        "    distr = np.ones(K) #the probability of each next_state (transition)\n",
        "    quit = np.random.binomial(1, q) #if quit == 1 then terminal state.\n",
        "\n",
        "    if(quit == 1):\n",
        "        return 0, 0, True\n",
        "\n",
        "    if(not all_relevant(state, action)): #at least one irrelevant video\n",
        "        distr *= (1/K) #user chooses next video randomly from search bar\n",
        "        next_state = np.random.choice(K, p = distr) #sample next state from the correct distribution\n",
        "    else: # all relevant\n",
        "        distr *= ((1-a)/K) #every video has a probability of (1-a)/K to be chosen\n",
        "        for item in action:\n",
        "            distr[item] += a/N #but those that are recommended also have an extra probability of a/N\n",
        "\n",
        "        next_state = np.random.choice(K, p = distr) #sample next state from the correct distribution\n",
        "\n",
        "    reward = is_cached(next_state) #if next_state is cached -> reward = 1 else 0\n",
        "\n",
        "    return next_state, reward, False\n",
        "\n",
        "\n",
        "def is_cached(item):\n",
        "#     return C[item]\n",
        "    return 1 if C[item] == 1 else 0\n",
        "\n",
        "def all_relevant(s, w):\n",
        "    for item in w:\n",
        "        if(not U_bool[s, item]): #if at least one of recommendations is not relevant return False\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "##################EVALUATION#######################\n",
        "def environment_model(state, action): #state is an integer, action is an N-tuple of recommendations\n",
        "    scenarios = []\n",
        "    if(all_relevant(state, action)):\n",
        "\n",
        "        prob = (1-q)*(a/N + (1-a)/K) #probability to choose any recommended video\n",
        "        for item in action:\n",
        "            reward = is_cached(item)\n",
        "            t = (prob, item, reward, False)\n",
        "            scenarios.append(t)\n",
        "\n",
        "        prob = (1-q)*((1-a)/K) #probability to choose any not recommended video\n",
        "        for item in range(K):\n",
        "            if(item in action):\n",
        "                continue\n",
        "            reward = is_cached(item)\n",
        "            t = (prob, item, reward, False)\n",
        "            scenarios.append(t)\n",
        "\n",
        "    else:\n",
        "        prob = (1-q)*((1-a)/K) #probability to choose any not recommended video\n",
        "        for item in range(K):\n",
        "            reward = is_cached(item)\n",
        "            t = (prob, item, reward, False)\n",
        "            scenarios.append(t)\n",
        "\n",
        "    t = (q, 0, 0, True) #terminal state, with probability q, terminate session\n",
        "    scenarios.append(t)\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "\n",
        "def generate_improved_policy(Q):\n",
        "    new_pi = np.zeros((K, N), dtype=np.int8)\n",
        "    # action_idxs = np.argmax(Q, axis=1)\n",
        "    for s in range(K):\n",
        "        best_action_idx = np.argmax(Q[s, :])\n",
        "        new_pi[s, :] = actions[best_action_idx]\n",
        "    return new_pi\n",
        "\n",
        "def generate_random_policy():\n",
        "    pi = np.zeros((K, N), dtype=np.int8)\n",
        "    for s in range(K):\n",
        "        action = np.random.choice(K, size=N, replace=False)\n",
        "        pi[s, :] = action\n",
        "    return pi\n",
        "\n",
        "\n",
        "\n",
        "def policy_evaluation(pi, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
        "    prev_V = np.zeros(K) # use as \"cost-to-go\", i.e. for V(s')\n",
        "    while True: #performing iterations\n",
        "        V = np.zeros(K) # current value function to be learnerd\n",
        "        for s in range(K):  # do for every state\n",
        "            for prob, next_state, reward, done in environment_model(s, pi[s, :]):  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
        "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
        "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one;\n",
        "            break # if yes, finish loop\n",
        "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_improvement(V, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
        "    Q = np.zeros((K, len(actions)), dtype=np.float64) #create a Q value array\n",
        "    for s in range(K):        # for every state in the environment/model\n",
        "        for i,a in enumerate(actions):  # and for every action in that state\n",
        "            for prob, next_state, reward, done in environment_model(s, a):  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve)\n",
        "                Q[s][i] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "    new_pi = generate_improved_policy(Q)\n",
        "\n",
        "    return Q, new_pi\n",
        "\n",
        "\n",
        "def policy_iteration(gamma = 1.0, epsilon = 1e-10):\n",
        "    t = 0\n",
        "    pi = generate_random_policy()\n",
        "\n",
        "    while True:\n",
        "        old_pi = pi  #keep the old policy to compare with new\n",
        "        V = policy_evaluation(pi, gamma, epsilon)   #evaluate latest policy --> you receive its converged value function\n",
        "        Q, pi = policy_improvement(V,gamma)         #get a better policy using the value function of the previous one just calculated\n",
        "\n",
        "        t += 1\n",
        "\n",
        "        if(np.array_equal(old_pi, pi)):# you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
        "            break\n",
        "\n",
        "    return V,pi\n",
        "\n",
        "\n",
        "def play_episodes(n, pi):\n",
        "    total_reward = 0\n",
        "    for i in range(n): #play n episodes\n",
        "        state = np.random.randint(0,K)\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = pi[state, :]\n",
        "            next_state, reward, done = env(state, action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "    avg_reward = total_reward / n\n",
        "    print(f'Policy iteration acheived: {avg_reward}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "eval_episodes = 10000\n",
        "epochs = 10\n",
        "batch_size = 1000\n",
        "\n",
        "# V,pi = policy_iteration(gamma=0.9)\n",
        "# play_episodes(eval_episodes, pi)\n",
        "\n",
        "pg_epoch_avg_ret = train(epochs=epochs, batch_size = batch_size, learning_rate=0.01, eval=True, eval_episodes = eval_episodes)\n",
        "q_epoch_avg_ret = q_learning(gamma=0.9, epochs=epochs, batch_size=batch_size, eval=True, eval_episodes = eval_episodes)\n",
        "\n",
        "plot_pg_q_same_graph(pg_epoch_avg_ret, q_epoch_avg_ret)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}